{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea950d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropogation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f246f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "iteration: 0 :::: [[ 0.17290511 -0.96067776]]\n",
      "###output######## [[0.82709489 0.96067776]]\n",
      "**********************\n",
      "iteration: 1 :::: [[ 0.17225143 -0.96044821]]\n",
      "###output######## [[0.82774857 0.96044821]]\n",
      "**********************\n",
      "iteration: 2 :::: [[ 0.17160434 -0.96021598]]\n",
      "###output######## [[0.82839566 0.96021598]]\n",
      "**********************\n",
      "iteration: 3 :::: [[ 0.17096376 -0.95998105]]\n",
      "###output######## [[0.82903624 0.95998105]]\n",
      "**********************\n",
      "iteration: 4 :::: [[ 0.17032957 -0.95974336]]\n",
      "###output######## [[0.82967043 0.95974336]]\n",
      "**********************\n",
      "iteration: 5 :::: [[ 0.16970169 -0.95950288]]\n",
      "###output######## [[0.83029831 0.95950288]]\n",
      "**********************\n",
      "iteration: 6 :::: [[ 0.16908001 -0.95925955]]\n",
      "###output######## [[0.83091999 0.95925955]]\n",
      "**********************\n",
      "iteration: 7 :::: [[ 0.16846444 -0.95901334]]\n",
      "###output######## [[0.83153556 0.95901334]]\n",
      "**********************\n",
      "iteration: 8 :::: [[ 0.1678549  -0.95876419]]\n",
      "###output######## [[0.8321451  0.95876419]]\n",
      "**********************\n",
      "iteration: 9 :::: [[ 0.16725129 -0.95851206]]\n",
      "###output######## [[0.83274871 0.95851206]]\n",
      "**********************\n",
      "iteration: 10 :::: [[ 0.16665351 -0.9582569 ]]\n",
      "###output######## [[0.83334649 0.9582569 ]]\n",
      "**********************\n",
      "iteration: 11 :::: [[ 0.1660615  -0.95799866]]\n",
      "###output######## [[0.8339385  0.95799866]]\n",
      "**********************\n",
      "iteration: 12 :::: [[ 0.16547516 -0.95773729]]\n",
      "###output######## [[0.83452484 0.95773729]]\n",
      "**********************\n",
      "iteration: 13 :::: [[ 0.1648944  -0.95747274]]\n",
      "###output######## [[0.8351056  0.95747274]]\n",
      "**********************\n",
      "iteration: 14 :::: [[ 0.16431915 -0.95720495]]\n",
      "###output######## [[0.83568085 0.95720495]]\n",
      "**********************\n",
      "iteration: 15 :::: [[ 0.16374933 -0.95693388]]\n",
      "###output######## [[0.83625067 0.95693388]]\n",
      "**********************\n",
      "iteration: 16 :::: [[ 0.16318485 -0.95665946]]\n",
      "###output######## [[0.83681515 0.95665946]]\n",
      "**********************\n",
      "iteration: 17 :::: [[ 0.16262564 -0.95638164]]\n",
      "###output######## [[0.83737436 0.95638164]]\n",
      "**********************\n",
      "iteration: 18 :::: [[ 0.16207162 -0.95610037]]\n",
      "###output######## [[0.83792838 0.95610037]]\n",
      "**********************\n",
      "iteration: 19 :::: [[ 0.16152273 -0.95581557]]\n",
      "###output######## [[0.83847727 0.95581557]]\n",
      "**********************\n",
      "iteration: 20 :::: [[ 0.16097888 -0.95552721]]\n",
      "###output######## [[0.83902112 0.95552721]]\n",
      "**********************\n",
      "iteration: 21 :::: [[ 0.16044   -0.9552352]]\n",
      "###output######## [[0.83956   0.9552352]]\n",
      "**********************\n",
      "iteration: 22 :::: [[ 0.15990602 -0.95493949]]\n",
      "###output######## [[0.84009398 0.95493949]]\n",
      "**********************\n",
      "iteration: 23 :::: [[ 0.15937688 -0.95464002]]\n",
      "###output######## [[0.84062312 0.95464002]]\n",
      "**********************\n",
      "iteration: 24 :::: [[ 0.1588525  -0.95433672]]\n",
      "###output######## [[0.8411475  0.95433672]]\n",
      "**********************\n",
      "iteration: 25 :::: [[ 0.15833282 -0.95402951]]\n",
      "###output######## [[0.84166718 0.95402951]]\n",
      "**********************\n",
      "iteration: 26 :::: [[ 0.15781777 -0.95371834]]\n",
      "###output######## [[0.84218223 0.95371834]]\n",
      "**********************\n",
      "iteration: 27 :::: [[ 0.15730729 -0.95340313]]\n",
      "###output######## [[0.84269271 0.95340313]]\n",
      "**********************\n",
      "iteration: 28 :::: [[ 0.15680132 -0.95308381]]\n",
      "###output######## [[0.84319868 0.95308381]]\n",
      "**********************\n",
      "iteration: 29 :::: [[ 0.15629978 -0.95276031]]\n",
      "###output######## [[0.84370022 0.95276031]]\n",
      "**********************\n",
      "iteration: 30 :::: [[ 0.15580262 -0.95243254]]\n",
      "###output######## [[0.84419738 0.95243254]]\n",
      "**********************\n",
      "iteration: 31 :::: [[ 0.15530978 -0.95210044]]\n",
      "###output######## [[0.84469022 0.95210044]]\n",
      "**********************\n",
      "iteration: 32 :::: [[ 0.1548212  -0.95176392]]\n",
      "###output######## [[0.8451788  0.95176392]]\n",
      "**********************\n",
      "iteration: 33 :::: [[ 0.15433682 -0.95142291]]\n",
      "###output######## [[0.84566318 0.95142291]]\n",
      "**********************\n",
      "iteration: 34 :::: [[ 0.15385659 -0.95107731]]\n",
      "###output######## [[0.84614341 0.95107731]]\n",
      "**********************\n",
      "iteration: 35 :::: [[ 0.15338044 -0.95072705]]\n",
      "###output######## [[0.84661956 0.95072705]]\n",
      "**********************\n",
      "iteration: 36 :::: [[ 0.15290832 -0.95037203]]\n",
      "###output######## [[0.84709168 0.95037203]]\n",
      "**********************\n",
      "iteration: 37 :::: [[ 0.15244018 -0.95001218]]\n",
      "###output######## [[0.84755982 0.95001218]]\n",
      "**********************\n",
      "iteration: 38 :::: [[ 0.15197596 -0.94964739]]\n",
      "###output######## [[0.84802404 0.94964739]]\n",
      "**********************\n",
      "iteration: 39 :::: [[ 0.15151561 -0.94927757]]\n",
      "###output######## [[0.84848439 0.94927757]]\n",
      "**********************\n",
      "iteration: 40 :::: [[ 0.15105908 -0.94890264]]\n",
      "###output######## [[0.84894092 0.94890264]]\n",
      "**********************\n",
      "iteration: 41 :::: [[ 0.15060631 -0.94852248]]\n",
      "###output######## [[0.84939369 0.94852248]]\n",
      "**********************\n",
      "iteration: 42 :::: [[ 0.15015726 -0.948137  ]]\n",
      "###output######## [[0.84984274 0.948137  ]]\n",
      "**********************\n",
      "iteration: 43 :::: [[ 0.14971187 -0.9477461 ]]\n",
      "###output######## [[0.85028813 0.9477461 ]]\n",
      "**********************\n",
      "iteration: 44 :::: [[ 0.14927009 -0.94734967]]\n",
      "###output######## [[0.85072991 0.94734967]]\n",
      "**********************\n",
      "iteration: 45 :::: [[ 0.14883189 -0.94694761]]\n",
      "###output######## [[0.85116811 0.94694761]]\n",
      "**********************\n",
      "iteration: 46 :::: [[ 0.1483972 -0.9465398]]\n",
      "###output######## [[0.8516028 0.9465398]]\n",
      "**********************\n",
      "iteration: 47 :::: [[ 0.14796599 -0.94612613]]\n",
      "###output######## [[0.85203401 0.94612613]]\n",
      "**********************\n",
      "iteration: 48 :::: [[ 0.1475382  -0.94570648]]\n",
      "###output######## [[0.8524618  0.94570648]]\n",
      "**********************\n",
      "iteration: 49 :::: [[ 0.14711379 -0.94528073]]\n",
      "###output######## [[0.85288621 0.94528073]]\n",
      "**********************\n",
      "iteration: 5951 :::: [[ 0.02114487 -0.02206585]]\n",
      "###output######## [[0.97885513 0.02206585]]\n",
      "**********************\n",
      "iteration: 5952 :::: [[ 0.02114308 -0.02206382]]\n",
      "###output######## [[0.97885692 0.02206382]]\n",
      "**********************\n",
      "iteration: 5953 :::: [[ 0.02114129 -0.0220618 ]]\n",
      "###output######## [[0.97885871 0.0220618 ]]\n",
      "**********************\n",
      "iteration: 5954 :::: [[ 0.0211395  -0.02205977]]\n",
      "###output######## [[0.9788605  0.02205977]]\n",
      "**********************\n",
      "iteration: 5955 :::: [[ 0.02113771 -0.02205774]]\n",
      "###output######## [[0.97886229 0.02205774]]\n",
      "**********************\n",
      "iteration: 5956 :::: [[ 0.02113592 -0.02205572]]\n",
      "###output######## [[0.97886408 0.02205572]]\n",
      "**********************\n",
      "iteration: 5957 :::: [[ 0.02113413 -0.02205369]]\n",
      "###output######## [[0.97886587 0.02205369]]\n",
      "**********************\n",
      "iteration: 5958 :::: [[ 0.02113234 -0.02205167]]\n",
      "###output######## [[0.97886766 0.02205167]]\n",
      "**********************\n",
      "iteration: 5959 :::: [[ 0.02113056 -0.02204964]]\n",
      "###output######## [[0.97886944 0.02204964]]\n",
      "**********************\n",
      "iteration: 5960 :::: [[ 0.02112877 -0.02204762]]\n",
      "###output######## [[0.97887123 0.02204762]]\n",
      "**********************\n",
      "iteration: 5961 :::: [[ 0.02112698 -0.0220456 ]]\n",
      "###output######## [[0.97887302 0.0220456 ]]\n",
      "**********************\n",
      "iteration: 5962 :::: [[ 0.0211252  -0.02204357]]\n",
      "###output######## [[0.9788748  0.02204357]]\n",
      "**********************\n",
      "iteration: 5963 :::: [[ 0.02112341 -0.02204155]]\n",
      "###output######## [[0.97887659 0.02204155]]\n",
      "**********************\n",
      "iteration: 5964 :::: [[ 0.02112162 -0.02203953]]\n",
      "###output######## [[0.97887838 0.02203953]]\n",
      "**********************\n",
      "iteration: 5965 :::: [[ 0.02111984 -0.02203751]]\n",
      "###output######## [[0.97888016 0.02203751]]\n",
      "**********************\n",
      "iteration: 5966 :::: [[ 0.02111805 -0.02203549]]\n",
      "###output######## [[0.97888195 0.02203549]]\n",
      "**********************\n",
      "iteration: 5967 :::: [[ 0.02111627 -0.02203347]]\n",
      "###output######## [[0.97888373 0.02203347]]\n",
      "**********************\n",
      "iteration: 5968 :::: [[ 0.02111449 -0.02203145]]\n",
      "###output######## [[0.97888551 0.02203145]]\n",
      "**********************\n",
      "iteration: 5969 :::: [[ 0.0211127  -0.02202943]]\n",
      "###output######## [[0.9788873  0.02202943]]\n",
      "**********************\n",
      "iteration: 5970 :::: [[ 0.02111092 -0.02202741]]\n",
      "###output######## [[0.97888908 0.02202741]]\n",
      "**********************\n",
      "iteration: 5971 :::: [[ 0.02110914 -0.0220254 ]]\n",
      "###output######## [[0.97889086 0.0220254 ]]\n",
      "**********************\n",
      "iteration: 5972 :::: [[ 0.02110736 -0.02202338]]\n",
      "###output######## [[0.97889264 0.02202338]]\n",
      "**********************\n",
      "iteration: 5973 :::: [[ 0.02110557 -0.02202136]]\n",
      "###output######## [[0.97889443 0.02202136]]\n",
      "**********************\n",
      "iteration: 5974 :::: [[ 0.02110379 -0.02201935]]\n",
      "###output######## [[0.97889621 0.02201935]]\n",
      "**********************\n",
      "iteration: 5975 :::: [[ 0.02110201 -0.02201733]]\n",
      "###output######## [[0.97889799 0.02201733]]\n",
      "**********************\n",
      "iteration: 5976 :::: [[ 0.02110023 -0.02201532]]\n",
      "###output######## [[0.97889977 0.02201532]]\n",
      "**********************\n",
      "iteration: 5977 :::: [[ 0.02109845 -0.0220133 ]]\n",
      "###output######## [[0.97890155 0.0220133 ]]\n",
      "**********************\n",
      "iteration: 5978 :::: [[ 0.02109667 -0.02201129]]\n",
      "###output######## [[0.97890333 0.02201129]]\n",
      "**********************\n",
      "iteration: 5979 :::: [[ 0.02109489 -0.02200928]]\n",
      "###output######## [[0.97890511 0.02200928]]\n",
      "**********************\n",
      "iteration: 5980 :::: [[ 0.02109312 -0.02200726]]\n",
      "###output######## [[0.97890688 0.02200726]]\n",
      "**********************\n",
      "iteration: 5981 :::: [[ 0.02109134 -0.02200525]]\n",
      "###output######## [[0.97890866 0.02200525]]\n",
      "**********************\n",
      "iteration: 5982 :::: [[ 0.02108956 -0.02200324]]\n",
      "###output######## [[0.97891044 0.02200324]]\n",
      "**********************\n",
      "iteration: 5983 :::: [[ 0.02108778 -0.02200123]]\n",
      "###output######## [[0.97891222 0.02200123]]\n",
      "**********************\n",
      "iteration: 5984 :::: [[ 0.02108601 -0.02199922]]\n",
      "###output######## [[0.97891399 0.02199922]]\n",
      "**********************\n",
      "iteration: 5985 :::: [[ 0.02108423 -0.02199721]]\n",
      "###output######## [[0.97891577 0.02199721]]\n",
      "**********************\n",
      "iteration: 5986 :::: [[ 0.02108245 -0.0219952 ]]\n",
      "###output######## [[0.97891755 0.0219952 ]]\n",
      "**********************\n",
      "iteration: 5987 :::: [[ 0.02108068 -0.02199319]]\n",
      "###output######## [[0.97891932 0.02199319]]\n",
      "**********************\n",
      "iteration: 5988 :::: [[ 0.0210789  -0.02199118]]\n",
      "###output######## [[0.9789211  0.02199118]]\n",
      "**********************\n",
      "iteration: 5989 :::: [[ 0.02107713 -0.02198917]]\n",
      "###output######## [[0.97892287 0.02198917]]\n",
      "**********************\n",
      "iteration: 5990 :::: [[ 0.02107536 -0.02198717]]\n",
      "###output######## [[0.97892464 0.02198717]]\n",
      "**********************\n",
      "iteration: 5991 :::: [[ 0.02107358 -0.02198516]]\n",
      "###output######## [[0.97892642 0.02198516]]\n",
      "**********************\n",
      "iteration: 5992 :::: [[ 0.02107181 -0.02198315]]\n",
      "###output######## [[0.97892819 0.02198315]]\n",
      "**********************\n",
      "iteration: 5993 :::: [[ 0.02107004 -0.02198115]]\n",
      "###output######## [[0.97892996 0.02198115]]\n",
      "**********************\n",
      "iteration: 5994 :::: [[ 0.02106826 -0.02197914]]\n",
      "###output######## [[0.97893174 0.02197914]]\n",
      "**********************\n",
      "iteration: 5995 :::: [[ 0.02106649 -0.02197714]]\n",
      "###output######## [[0.97893351 0.02197714]]\n",
      "**********************\n",
      "iteration: 5996 :::: [[ 0.02106472 -0.02197513]]\n",
      "###output######## [[0.97893528 0.02197513]]\n",
      "**********************\n",
      "iteration: 5997 :::: [[ 0.02106295 -0.02197313]]\n",
      "###output######## [[0.97893705 0.02197313]]\n",
      "**********************\n",
      "iteration: 5998 :::: [[ 0.02106118 -0.02197113]]\n",
      "###output######## [[0.97893882 0.02197113]]\n",
      "**********************\n",
      "iteration: 5999 :::: [[ 0.02105941 -0.02196912]]\n",
      "###output######## [[0.97894059 0.02196912]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize hyperparameters\n",
    "inputNeurons = 2\n",
    "hiddenlayerNeurons = 4\n",
    "outputNeurons = 2\n",
    "iteration = 6000\n",
    "\n",
    "# Initialize input and output data\n",
    "input = np.random.randint(1, 5, inputNeurons)\n",
    "output = np.array([1.0, 0.0])\n",
    "\n",
    "# Initialize hidden and output layer weights and biases\n",
    "hidden_layer = np.random.rand(1, hiddenlayerNeurons)\n",
    "hidden_biass = np.random.rand(1, hiddenlayerNeurons)\n",
    "output_bias = np.random.rand(1, outputNeurons)\n",
    "hidden_weights = np.random.rand(inputNeurons, hiddenlayerNeurons)\n",
    "output_weights = np.random.rand(hiddenlayerNeurons, outputNeurons)\n",
    "\n",
    "def sigmoid(layer):\n",
    "    \"\"\"\n",
    "    Apply the sigmoid function element-wise to the input layer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    layer : array-like\n",
    "        The input layer.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array-like\n",
    "        The output of the sigmoid function applied element-wise to the input layer.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-layer))\n",
    "\n",
    "def gradient(layer): \n",
    "    \"\"\"\n",
    "    Calculate the gradient of the sigmoid function element-wise for the input layer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    layer : array-like\n",
    "        The input layer.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array-like\n",
    "        The gradient of the sigmoid function applied element-wise to the input layer.\n",
    "    \"\"\"\n",
    "    return layer * (1 - layer)\n",
    "\n",
    "# Loop through the specified number of iterations\n",
    "for i in range(iteration):\n",
    "    # Forward pass\n",
    "    hidden_layer = np.dot(input, hidden_weights)  # Linear combination in hidden layer\n",
    "    hidden_layer = sigmoid(hidden_layer + hidden_biass)  # Activation in hidden layer\n",
    "    output_layer = np.dot(hidden_layer, output_weights)  # Linear combination in output layer\n",
    "    output_layer = sigmoid(output_layer + output_bias)\n",
    "    \n",
    "    # Calculate error and error terms\n",
    "    error = (output - output_layer) \n",
    "    gradient_outputLayer = gradient(output_layer)\n",
    "    error_terms_output = gradient_outputLayer * error\n",
    "    error_terms_hidden = gradient(hidden_layer) * np.dot(error_terms_output, output_weights.T)\n",
    "    \n",
    "    # Calculate gradients for hidden and output weights\n",
    "    gradient_hidden_weights = np.dot(input.reshape(inputNeurons, 1), error_terms_hidden.reshape(1, hiddenlayerNeurons))\n",
    "    gradient_ouput_weights = np.dot(hidden_layer.reshape(hiddenlayerNeurons, 1), error_terms_output.reshape(1, outputNeurons))\n",
    "    \n",
    "    # Update hidden and output weights\n",
    "    hidden_weights = hidden_weights + 0.05 * gradient_hidden_weights\n",
    "    output_weights = output_weights + 0.05 * gradient_ouput_weights\n",
    "\n",
    "# Print information at the beginning and end of training\n",
    "    if i < 50 or i > iteration - 50:\n",
    "        print(\"**********************\") \n",
    "        print(\"iteration:\", i, \"::::\", error) \n",
    "        print(\"###output########\", output_layer)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
