{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea950d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropogation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f246f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "iteration: 0 :::: [[ 0.14183871 -0.84915958]]\n",
      "###output######## [[0.85816129 0.84915958]]\n",
      "**********************\n",
      "iteration: 1 :::: [[ 0.14154488 -0.84679077]]\n",
      "###output######## [[0.85845512 0.84679077]]\n",
      "**********************\n",
      "iteration: 2 :::: [[ 0.14125344 -0.844369  ]]\n",
      "###output######## [[0.85874656 0.844369  ]]\n",
      "**********************\n",
      "iteration: 3 :::: [[ 0.14096435 -0.84189313]]\n",
      "###output######## [[0.85903565 0.84189313]]\n",
      "**********************\n",
      "iteration: 4 :::: [[ 0.14067759 -0.83936202]]\n",
      "###output######## [[0.85932241 0.83936202]]\n",
      "**********************\n",
      "iteration: 5 :::: [[ 0.14039312 -0.83677453]]\n",
      "###output######## [[0.85960688 0.83677453]]\n",
      "**********************\n",
      "iteration: 6 :::: [[ 0.14011091 -0.83412951]]\n",
      "###output######## [[0.85988909 0.83412951]]\n",
      "**********************\n",
      "iteration: 7 :::: [[ 0.13983093 -0.83142583]]\n",
      "###output######## [[0.86016907 0.83142583]]\n",
      "**********************\n",
      "iteration: 8 :::: [[ 0.13955316 -0.82866236]]\n",
      "###output######## [[0.86044684 0.82866236]]\n",
      "**********************\n",
      "iteration: 9 :::: [[ 0.13927754 -0.82583799]]\n",
      "###output######## [[0.86072246 0.82583799]]\n",
      "**********************\n",
      "iteration: 10 :::: [[ 0.13900406 -0.82295163]]\n",
      "###output######## [[0.86099594 0.82295163]]\n",
      "**********************\n",
      "iteration: 11 :::: [[ 0.13873268 -0.82000218]]\n",
      "###output######## [[0.86126732 0.82000218]]\n",
      "**********************\n",
      "iteration: 12 :::: [[ 0.13846335 -0.81698859]]\n",
      "###output######## [[0.86153665 0.81698859]]\n",
      "**********************\n",
      "iteration: 13 :::: [[ 0.13819606 -0.81390983]]\n",
      "###output######## [[0.86180394 0.81390983]]\n",
      "**********************\n",
      "iteration: 14 :::: [[ 0.13793075 -0.81076491]]\n",
      "###output######## [[0.86206925 0.81076491]]\n",
      "**********************\n",
      "iteration: 15 :::: [[ 0.1376674  -0.80755287]]\n",
      "###output######## [[0.8623326  0.80755287]]\n",
      "**********************\n",
      "iteration: 16 :::: [[ 0.13740597 -0.80427279]]\n",
      "###output######## [[0.86259403 0.80427279]]\n",
      "**********************\n",
      "iteration: 17 :::: [[ 0.13714641 -0.8009238 ]]\n",
      "###output######## [[0.86285359 0.8009238 ]]\n",
      "**********************\n",
      "iteration: 18 :::: [[ 0.1368887  -0.79750508]]\n",
      "###output######## [[0.8631113  0.79750508]]\n",
      "**********************\n",
      "iteration: 19 :::: [[ 0.13663278 -0.79401589]]\n",
      "###output######## [[0.86336722 0.79401589]]\n",
      "**********************\n",
      "iteration: 20 :::: [[ 0.13637863 -0.79045553]]\n",
      "###output######## [[0.86362137 0.79045553]]\n",
      "**********************\n",
      "iteration: 21 :::: [[ 0.13612619 -0.78682338]]\n",
      "###output######## [[0.86387381 0.78682338]]\n",
      "**********************\n",
      "iteration: 22 :::: [[ 0.13587544 -0.78311891]]\n",
      "###output######## [[0.86412456 0.78311891]]\n",
      "**********************\n",
      "iteration: 23 :::: [[ 0.13562631 -0.77934166]]\n",
      "###output######## [[0.86437369 0.77934166]]\n",
      "**********************\n",
      "iteration: 24 :::: [[ 0.13537878 -0.77549127]]\n",
      "###output######## [[0.86462122 0.77549127]]\n",
      "**********************\n",
      "iteration: 25 :::: [[ 0.1351328  -0.77156748]]\n",
      "###output######## [[0.8648672  0.77156748]]\n",
      "**********************\n",
      "iteration: 26 :::: [[ 0.13488832 -0.76757013]]\n",
      "###output######## [[0.86511168 0.76757013]]\n",
      "**********************\n",
      "iteration: 27 :::: [[ 0.1346453  -0.76349916]]\n",
      "###output######## [[0.8653547  0.76349916]]\n",
      "**********************\n",
      "iteration: 28 :::: [[ 0.13440369 -0.75935467]]\n",
      "###output######## [[0.86559631 0.75935467]]\n",
      "**********************\n",
      "iteration: 29 :::: [[ 0.13416345 -0.75513684]]\n",
      "###output######## [[0.86583655 0.75513684]]\n",
      "**********************\n",
      "iteration: 30 :::: [[ 0.13392454 -0.75084599]]\n",
      "###output######## [[0.86607546 0.75084599]]\n",
      "**********************\n",
      "iteration: 31 :::: [[ 0.13368689 -0.74648261]]\n",
      "###output######## [[0.86631311 0.74648261]]\n",
      "**********************\n",
      "iteration: 32 :::: [[ 0.13345047 -0.74204728]]\n",
      "###output######## [[0.86654953 0.74204728]]\n",
      "**********************\n",
      "iteration: 33 :::: [[ 0.13321524 -0.73754077]]\n",
      "###output######## [[0.86678476 0.73754077]]\n",
      "**********************\n",
      "iteration: 34 :::: [[ 0.13298113 -0.73296399]]\n",
      "###output######## [[0.86701887 0.73296399]]\n",
      "**********************\n",
      "iteration: 35 :::: [[ 0.13274811 -0.72831799]]\n",
      "###output######## [[0.86725189 0.72831799]]\n",
      "**********************\n",
      "iteration: 36 :::: [[ 0.13251612 -0.723604  ]]\n",
      "###output######## [[0.86748388 0.723604  ]]\n",
      "**********************\n",
      "iteration: 37 :::: [[ 0.13228513 -0.71882339]]\n",
      "###output######## [[0.86771487 0.71882339]]\n",
      "**********************\n",
      "iteration: 38 :::: [[ 0.13205507 -0.71397771]]\n",
      "###output######## [[0.86794493 0.71397771]]\n",
      "**********************\n",
      "iteration: 39 :::: [[ 0.13182591 -0.70906866]]\n",
      "###output######## [[0.86817409 0.70906866]]\n",
      "**********************\n",
      "iteration: 40 :::: [[ 0.1315976  -0.70409812]]\n",
      "###output######## [[0.8684024  0.70409812]]\n",
      "**********************\n",
      "iteration: 41 :::: [[ 0.13137008 -0.69906813]]\n",
      "###output######## [[0.86862992 0.69906813]]\n",
      "**********************\n",
      "iteration: 42 :::: [[ 0.13114333 -0.69398086]]\n",
      "###output######## [[0.86885667 0.69398086]]\n",
      "**********************\n",
      "iteration: 43 :::: [[ 0.13091728 -0.68883867]]\n",
      "###output######## [[0.86908272 0.68883867]]\n",
      "**********************\n",
      "iteration: 44 :::: [[ 0.13069191 -0.68364407]]\n",
      "###output######## [[0.86930809 0.68364407]]\n",
      "**********************\n",
      "iteration: 45 :::: [[ 0.13046716 -0.67839969]]\n",
      "###output######## [[0.86953284 0.67839969]]\n",
      "**********************\n",
      "iteration: 46 :::: [[ 0.13024299 -0.67310833]]\n",
      "###output######## [[0.86975701 0.67310833]]\n",
      "**********************\n",
      "iteration: 47 :::: [[ 0.13001937 -0.6677729 ]]\n",
      "###output######## [[0.86998063 0.6677729 ]]\n",
      "**********************\n",
      "iteration: 48 :::: [[ 0.12979625 -0.66239645]]\n",
      "###output######## [[0.87020375 0.66239645]]\n",
      "**********************\n",
      "iteration: 49 :::: [[ 0.1295736  -0.65698215]]\n",
      "###output######## [[0.8704264  0.65698215]]\n",
      "**********************\n",
      "iteration: 5951 :::: [[ 0.02188709 -0.02276138]]\n",
      "###output######## [[0.97811291 0.02276138]]\n",
      "**********************\n",
      "iteration: 5952 :::: [[ 0.0218852  -0.02275929]]\n",
      "###output######## [[0.9781148  0.02275929]]\n",
      "**********************\n",
      "iteration: 5953 :::: [[ 0.0218833 -0.0227572]]\n",
      "###output######## [[0.9781167 0.0227572]]\n",
      "**********************\n",
      "iteration: 5954 :::: [[ 0.0218814  -0.02275511]]\n",
      "###output######## [[0.9781186  0.02275511]]\n",
      "**********************\n",
      "iteration: 5955 :::: [[ 0.02187951 -0.02275303]]\n",
      "###output######## [[0.97812049 0.02275303]]\n",
      "**********************\n",
      "iteration: 5956 :::: [[ 0.02187761 -0.02275094]]\n",
      "###output######## [[0.97812239 0.02275094]]\n",
      "**********************\n",
      "iteration: 5957 :::: [[ 0.02187572 -0.02274885]]\n",
      "###output######## [[0.97812428 0.02274885]]\n",
      "**********************\n",
      "iteration: 5958 :::: [[ 0.02187382 -0.02274677]]\n",
      "###output######## [[0.97812618 0.02274677]]\n",
      "**********************\n",
      "iteration: 5959 :::: [[ 0.02187193 -0.02274468]]\n",
      "###output######## [[0.97812807 0.02274468]]\n",
      "**********************\n",
      "iteration: 5960 :::: [[ 0.02187004 -0.0227426 ]]\n",
      "###output######## [[0.97812996 0.0227426 ]]\n",
      "**********************\n",
      "iteration: 5961 :::: [[ 0.02186814 -0.02274051]]\n",
      "###output######## [[0.97813186 0.02274051]]\n",
      "**********************\n",
      "iteration: 5962 :::: [[ 0.02186625 -0.02273843]]\n",
      "###output######## [[0.97813375 0.02273843]]\n",
      "**********************\n",
      "iteration: 5963 :::: [[ 0.02186436 -0.02273635]]\n",
      "###output######## [[0.97813564 0.02273635]]\n",
      "**********************\n",
      "iteration: 5964 :::: [[ 0.02186247 -0.02273426]]\n",
      "###output######## [[0.97813753 0.02273426]]\n",
      "**********************\n",
      "iteration: 5965 :::: [[ 0.02186058 -0.02273218]]\n",
      "###output######## [[0.97813942 0.02273218]]\n",
      "**********************\n",
      "iteration: 5966 :::: [[ 0.02185869 -0.0227301 ]]\n",
      "###output######## [[0.97814131 0.0227301 ]]\n",
      "**********************\n",
      "iteration: 5967 :::: [[ 0.0218568  -0.02272802]]\n",
      "###output######## [[0.9781432  0.02272802]]\n",
      "**********************\n",
      "iteration: 5968 :::: [[ 0.02185491 -0.02272594]]\n",
      "###output######## [[0.97814509 0.02272594]]\n",
      "**********************\n",
      "iteration: 5969 :::: [[ 0.02185302 -0.02272386]]\n",
      "###output######## [[0.97814698 0.02272386]]\n",
      "**********************\n",
      "iteration: 5970 :::: [[ 0.02185113 -0.02272178]]\n",
      "###output######## [[0.97814887 0.02272178]]\n",
      "**********************\n",
      "iteration: 5971 :::: [[ 0.02184924 -0.0227197 ]]\n",
      "###output######## [[0.97815076 0.0227197 ]]\n",
      "**********************\n",
      "iteration: 5972 :::: [[ 0.02184736 -0.02271762]]\n",
      "###output######## [[0.97815264 0.02271762]]\n",
      "**********************\n",
      "iteration: 5973 :::: [[ 0.02184547 -0.02271555]]\n",
      "###output######## [[0.97815453 0.02271555]]\n",
      "**********************\n",
      "iteration: 5974 :::: [[ 0.02184358 -0.02271347]]\n",
      "###output######## [[0.97815642 0.02271347]]\n",
      "**********************\n",
      "iteration: 5975 :::: [[ 0.0218417  -0.02271139]]\n",
      "###output######## [[0.9781583  0.02271139]]\n",
      "**********************\n",
      "iteration: 5976 :::: [[ 0.02183981 -0.02270932]]\n",
      "###output######## [[0.97816019 0.02270932]]\n",
      "**********************\n",
      "iteration: 5977 :::: [[ 0.02183793 -0.02270724]]\n",
      "###output######## [[0.97816207 0.02270724]]\n",
      "**********************\n",
      "iteration: 5978 :::: [[ 0.02183604 -0.02270517]]\n",
      "###output######## [[0.97816396 0.02270517]]\n",
      "**********************\n",
      "iteration: 5979 :::: [[ 0.02183416 -0.02270309]]\n",
      "###output######## [[0.97816584 0.02270309]]\n",
      "**********************\n",
      "iteration: 5980 :::: [[ 0.02183227 -0.02270102]]\n",
      "###output######## [[0.97816773 0.02270102]]\n",
      "**********************\n",
      "iteration: 5981 :::: [[ 0.02183039 -0.02269895]]\n",
      "###output######## [[0.97816961 0.02269895]]\n",
      "**********************\n",
      "iteration: 5982 :::: [[ 0.02182851 -0.02269687]]\n",
      "###output######## [[0.97817149 0.02269687]]\n",
      "**********************\n",
      "iteration: 5983 :::: [[ 0.02182663 -0.0226948 ]]\n",
      "###output######## [[0.97817337 0.0226948 ]]\n",
      "**********************\n",
      "iteration: 5984 :::: [[ 0.02182475 -0.02269273]]\n",
      "###output######## [[0.97817525 0.02269273]]\n",
      "**********************\n",
      "iteration: 5985 :::: [[ 0.02182286 -0.02269066]]\n",
      "###output######## [[0.97817714 0.02269066]]\n",
      "**********************\n",
      "iteration: 5986 :::: [[ 0.02182098 -0.02268859]]\n",
      "###output######## [[0.97817902 0.02268859]]\n",
      "**********************\n",
      "iteration: 5987 :::: [[ 0.0218191  -0.02268652]]\n",
      "###output######## [[0.9781809  0.02268652]]\n",
      "**********************\n",
      "iteration: 5988 :::: [[ 0.02181722 -0.02268445]]\n",
      "###output######## [[0.97818278 0.02268445]]\n",
      "**********************\n",
      "iteration: 5989 :::: [[ 0.02181534 -0.02268238]]\n",
      "###output######## [[0.97818466 0.02268238]]\n",
      "**********************\n",
      "iteration: 5990 :::: [[ 0.02181346 -0.02268031]]\n",
      "###output######## [[0.97818654 0.02268031]]\n",
      "**********************\n",
      "iteration: 5991 :::: [[ 0.02181159 -0.02267824]]\n",
      "###output######## [[0.97818841 0.02267824]]\n",
      "**********************\n",
      "iteration: 5992 :::: [[ 0.02180971 -0.02267618]]\n",
      "###output######## [[0.97819029 0.02267618]]\n",
      "**********************\n",
      "iteration: 5993 :::: [[ 0.02180783 -0.02267411]]\n",
      "###output######## [[0.97819217 0.02267411]]\n",
      "**********************\n",
      "iteration: 5994 :::: [[ 0.02180595 -0.02267204]]\n",
      "###output######## [[0.97819405 0.02267204]]\n",
      "**********************\n",
      "iteration: 5995 :::: [[ 0.02180408 -0.02266998]]\n",
      "###output######## [[0.97819592 0.02266998]]\n",
      "**********************\n",
      "iteration: 5996 :::: [[ 0.0218022  -0.02266791]]\n",
      "###output######## [[0.9781978  0.02266791]]\n",
      "**********************\n",
      "iteration: 5997 :::: [[ 0.02180033 -0.02266585]]\n",
      "###output######## [[0.97819967 0.02266585]]\n",
      "**********************\n",
      "iteration: 5998 :::: [[ 0.02179845 -0.02266379]]\n",
      "###output######## [[0.97820155 0.02266379]]\n",
      "**********************\n",
      "iteration: 5999 :::: [[ 0.02179658 -0.02266172]]\n",
      "###output######## [[0.97820342 0.02266172]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize hyperparameters\n",
    "inputNeurons = 2\n",
    "hiddenlayerNeurons = 4\n",
    "outputNeurons = 2\n",
    "iteration = 6000\n",
    "\n",
    "# Initialize input and output data\n",
    "input = np.random.randint(1, 5, inputNeurons)\n",
    "output = np.array([1.0, 0.0])\n",
    "\n",
    "# Initialize hidden and output layer weights and biases\n",
    "hidden_layer = np.random.rand(1, hiddenlayerNeurons)\n",
    "hidden_bias = np.random.rand(1, hiddenlayerNeurons)\n",
    "output_bias = np.random.rand(1, outputNeurons)\n",
    "hidden_weights = np.random.rand(inputNeurons, hiddenlayerNeurons)\n",
    "output_weights = np.random.rand(hiddenlayerNeurons, outputNeurons)\n",
    "\n",
    "def sigmoid(layer):\n",
    "    \"\"\"\n",
    "    Apply the sigmoid function element-wise to the input layer.\n",
    "    \n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-layer))\n",
    "\n",
    "def gradient(layer): \n",
    "    \"\"\"\n",
    "    Calculate the gradient of the sigmoid function element-wise for the input layer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    layer : array-like\n",
    "        The input layer.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array-like\n",
    "        The gradient of the sigmoid function applied element-wise to the input layer.\n",
    "    \"\"\"\n",
    "    return layer * (1 - layer)\n",
    "\n",
    "# Loop through the specified number of iterations\n",
    "for i in range(iteration):\n",
    "    # Forward pass\n",
    "    hidden_layer = np.dot(input, hidden_weights)  # Linear combination in hidden layer\n",
    "    hidden_layer = sigmoid(hidden_layer + hidden_bias)  # Activation in hidden layer\n",
    "    output_layer = np.dot(hidden_layer, output_weights)  # Linear combination in output layer\n",
    "    output_layer = sigmoid(output_layer + output_bias)\n",
    "    \n",
    "    # Calculate error and error terms\n",
    "    error = (output - output_layer) \n",
    "    gradient_outputLayer = gradient(output_layer)\n",
    "    error_terms_output = gradient_outputLayer * error\n",
    "    error_terms_hidden = gradient(hidden_layer) * np.dot(error_terms_output, output_weights.T)\n",
    "    \n",
    "    # Calculate gradients for hidden and output weights\n",
    "    gradient_hidden_weights = np.dot(input.reshape(inputNeurons, 1), error_terms_hidden.reshape(1, hiddenlayerNeurons))\n",
    "    gradient_ouput_weights = np.dot(hidden_layer.reshape(hiddenlayerNeurons, 1), error_terms_output.reshape(1, outputNeurons))\n",
    "    \n",
    "    # Update hidden and output weights\n",
    "    hidden_weights = hidden_weights + 0.05 * gradient_hidden_weights\n",
    "    output_weights = output_weights + 0.05 * gradient_ouput_weights\n",
    "\n",
    "# Print information at the beginning and end of training\n",
    "    if i < 50 or i > iteration - 50:\n",
    "        print(\"**********************\") \n",
    "        print(\"iteration:\", i, \"::::\", error) \n",
    "        print(\"###output########\", output_layer)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
